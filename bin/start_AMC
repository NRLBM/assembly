#!/bin/bash

#echo "The script is being updated, this will be done momentarily..."
#exit

# This script checks some prerequisites before the actual pipeline commences
# The script should be in the bin folder and this folder should be added to PATH if the pipeline has to be started according to the manual.

# Set sensible bash defaults. See https://gist.github.com/mohanpedala/1e2ff5661761d3abd0385e8223e16425 for explanation.
set -euo pipefail

# Check if a file is present in which the user email is specified. Exit if this is not the case
if [ ! -f .email_user ]
then
  echo "There is no email address of the user provided in the .email_user file. Exiting"
  exit 1
fi

# Same but for the bioinformatician email
if [ ! -f .email_bioinformatician ]
then
  echo "There is no email address of the bioinformatician provided in the .email_bioinformatician file. Exiting"
  exit 1
fi

# check.py checks input files (whether every sample has a single R1 and a single R2 file present, which are not empty and whether no other files are present)
# also returns necessary calculation time (# of samples * 20 minutes)
NR_MINUTES=$(python workflow/scripts/check.py)

# Parse the timestamp which is passed as Snakemake configuration
START_TIME=$(date '+%Y.%m.%d_%H.%M.%S')

# Load conda environment
source ~/miniconda3/etc/profile.d/conda.sh
conda activate env_snakemake

# Use sed to edit the job template and add the timestamp to the script.
mkdir -p slurm/jobs slurm/slurm_logs/${START_TIME}
cp workflow/AMC_job_template.txt slurm/jobs/"job_AMC_${START_TIME}.txt"
sed -i "s/REPLACE_TIME/$START_TIME/" slurm/jobs/"job_AMC_${START_TIME}.txt"

# Submit the edited job with:
# Mail at end and error of analysis to bioinformatician
# Specified time, calculated by check.py
# Send error and output to slurm log
sbatch --mail-type END,ERROR --mail-user $(cat .email_bioinformatician) --time "$NR_MINUTES" --output "slurm/slurm_logs/${START_TIME}/job_%j_%u.out" --error "slurm/slurm_logs/${START_TIME}/job_%j_%u.err" slurm/jobs/"job_AMC_${START_TIME}.txt"
